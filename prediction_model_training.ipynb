{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Lazy Predict to test all models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 55.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 35, number of negative: 28\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24\n",
      "[LightGBM] [Info] Number of data points in the train set: 63, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.555556 -> initscore=0.223144\n",
      "[LightGBM] [Info] Start training from score 0.223144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "ExtraTreeClassifier                0.60               0.59     0.59      0.59   \n",
      "AdaBoostClassifier                 0.59               0.58     0.58      0.58   \n",
      "RandomForestClassifier             0.59               0.57     0.57      0.57   \n",
      "GaussianNB                         0.59               0.57     0.57      0.57   \n",
      "ExtraTreesClassifier               0.57               0.56     0.56      0.56   \n",
      "XGBClassifier                      0.57               0.56     0.56      0.56   \n",
      "NuSVC                              0.57               0.55     0.55      0.54   \n",
      "LGBMClassifier                     0.56               0.55     0.55      0.55   \n",
      "SVC                                0.57               0.54     0.54      0.51   \n",
      "BaggingClassifier                  0.56               0.54     0.54      0.54   \n",
      "NearestCentroid                    0.54               0.53     0.53      0.54   \n",
      "CalibratedClassifierCV             0.56               0.53     0.53      0.50   \n",
      "BernoulliNB                        0.54               0.53     0.53      0.53   \n",
      "LabelPropagation                   0.54               0.52     0.52      0.52   \n",
      "LabelSpreading                     0.54               0.52     0.52      0.52   \n",
      "KNeighborsClassifier               0.54               0.52     0.52      0.50   \n",
      "RidgeClassifierCV                  0.52               0.51     0.51      0.50   \n",
      "DummyClassifier                    0.54               0.50     0.50      0.38   \n",
      "LogisticRegression                 0.51               0.49     0.49      0.48   \n",
      "RidgeClassifier                    0.51               0.49     0.49      0.48   \n",
      "LinearSVC                          0.51               0.49     0.49      0.48   \n",
      "LinearDiscriminantAnalysis         0.51               0.49     0.49      0.48   \n",
      "DecisionTreeClassifier             0.49               0.48     0.48      0.48   \n",
      "PassiveAggressiveClassifier        0.51               0.48     0.48      0.44   \n",
      "SGDClassifier                      0.49               0.48     0.48      0.48   \n",
      "Perceptron                         0.51               0.48     0.48      0.41   \n",
      "QuadraticDiscriminantAnalysis      0.49               0.47     0.47      0.45   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "ExtraTreeClassifier                  0.01  \n",
      "AdaBoostClassifier                   0.05  \n",
      "RandomForestClassifier               0.06  \n",
      "GaussianNB                           0.01  \n",
      "ExtraTreesClassifier                 0.05  \n",
      "XGBClassifier                        0.11  \n",
      "NuSVC                                0.01  \n",
      "LGBMClassifier                       0.12  \n",
      "SVC                                  0.01  \n",
      "BaggingClassifier                    0.02  \n",
      "NearestCentroid                      0.01  \n",
      "CalibratedClassifierCV               0.02  \n",
      "BernoulliNB                          0.01  \n",
      "LabelPropagation                     0.01  \n",
      "LabelSpreading                       0.01  \n",
      "KNeighborsClassifier                 0.01  \n",
      "RidgeClassifierCV                    0.01  \n",
      "DummyClassifier                      0.01  \n",
      "LogisticRegression                   0.01  \n",
      "RidgeClassifier                      0.01  \n",
      "LinearSVC                            0.01  \n",
      "LinearDiscriminantAnalysis           0.01  \n",
      "DecisionTreeClassifier               0.01  \n",
      "PassiveAggressiveClassifier          0.01  \n",
      "SGDClassifier                        0.01  \n",
      "Perceptron                           0.01  \n",
      "QuadraticDiscriminantAnalysis        0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "seed = random.randint(1000, 9999)\n",
    "print(seed)\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"ACME-HappinessSurvey2020.csv\")\n",
    "\n",
    "# Define the features (X) and the target variable (y)\n",
    "X = df[['X1', 'X2', 'X3', 'X4', 'X5', 'X6']]\n",
    "y = df['Y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=.5, random_state=seed)\n",
    "\n",
    "# Initialize the LazyClassifier\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "# Fit the models and make predictions\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print the models and their performance\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "based on lazy predict analysis top 3 models are:\n",
    "\n",
    "PassiveAggressiveClassifier        0.63               0.61     0.61      0.61   \n",
    "XGBClassifier                      0.62               0.60     0.60      0.61   \n",
    "KNeighborsClassifier               0.60               0.59     0.59      0.60  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing accuracy of the top 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for PassiveAggressiveClassifier: {'C': 1, 'max_iter': 1000}\n",
      "Best score for PassiveAggressiveClassifier: 0.5800000000000001\n",
      "Accuracy on test data for PassiveAggressiveClassifier: 0.46153846153846156\n",
      "Best parameters for XGBClassifier: {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}\n",
      "Best score for XGBClassifier: 0.5900000000000001\n",
      "Accuracy on test data for XGBClassifier: 0.5769230769230769\n",
      "Best parameters for KNeighborsClassifier: {'algorithm': 'auto', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "Best score for KNeighborsClassifier: 0.58\n",
      "Accuracy on test data for KNeighborsClassifier: 0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the models and their hyperparameters\n",
    "models = {\n",
    "    'PassiveAggressiveClassifier': {\n",
    "        'model': PassiveAggressiveClassifier(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'max_iter': [1000, 2000, 3000]\n",
    "        }\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model': XGBClassifier(),\n",
    "        'params': {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.1, 0.5, 1],\n",
    "            'n_estimators': [50, 100, 200]\n",
    "        }\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        model['model'], model['params'], cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and the best score\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model_name}: {grid_search.best_score_}\")\n",
    "\n",
    "    # Train the model with the best parameters and evaluate on the test data\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(\n",
    "        f\"Accuracy on test data for {model_name}: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 , removing X2 feature since it has very little impact on the result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for PassiveAggressiveClassifier: {'C': 10, 'max_iter': 2000}\n",
      "Best score for PassiveAggressiveClassifier: 0.64\n",
      "Accuracy on test data for PassiveAggressiveClassifier: 0.38461538461538464\n",
      "Best parameters for XGBClassifier: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "Best score for XGBClassifier: 0.66\n",
      "Accuracy on test data for XGBClassifier: 0.6538461538461539\n",
      "Best parameters for KNeighborsClassifier: {'algorithm': 'auto', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "Best score for KNeighborsClassifier: 0.61\n",
      "Accuracy on test data for KNeighborsClassifier: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target variable\n",
    "X = df[['X1', 'X3', 'X4', 'X5', 'X6']]\n",
    "y = df['Y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models and their hyperparameters\n",
    "models = {\n",
    "    'PassiveAggressiveClassifier': {\n",
    "        'model': PassiveAggressiveClassifier(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'max_iter': [1000, 2000, 3000]\n",
    "        }\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model': XGBClassifier(),\n",
    "        'params': {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.1, 0.5, 1],\n",
    "            'n_estimators': [50, 100, 200]\n",
    "        }\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        model['model'], model['params'], cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and the best score\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model_name}: {grid_search.best_score_}\")\n",
    "\n",
    "    # Train the model with the best parameters and evaluate on the test data\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(\n",
    "        f\"Accuracy on test data for {model_name}: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
